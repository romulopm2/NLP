{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Análise de sentimentos com NLTK Introdução",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Step 1 - Install NLTK and downloading the data **"
      ],
      "metadata": {
        "id": "Ds5njkmvO4X1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3w_UFVYg4NUb",
        "outputId": "1f5fac13-9579-4d92-a4d3-5681c3e7d2f4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package twitter_samples to /root/nltk_data...\n",
            "[nltk_data]   Package twitter_samples is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.corpus import twitter_samples\n",
        "from nltk.tag import pos_tag\n",
        "import re, string \n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.wordnet import WordNetLemmatizer \n",
        "from nltk import FreqDist \n",
        "import random \n",
        "from nltk import classify\n",
        "from nltk import NaiveBayesClassifier\n",
        "from nltk.tokenize import word_tokenize \n",
        "nltk.download('twitter_samples')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 2 - Tokenize the data**"
      ],
      "metadata": {
        "id": "5UjD70GKO0wG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
        "negative_tweets = twitter_samples.strings('negative_tweets.json')\n",
        "text = twitter_samples.strings('tweets.20150430-223406.json')\n",
        "tweet_tokens = twitter_samples.tokenized('positive_tweets.json')\n",
        "# print(tweet_tokens[0])\n"
      ],
      "metadata": {
        "id": "Ia7JUjej4Zu1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tweet_tokens = twitter_samples.tokenized('positive_tweets.json')\n",
        "print(pos_tag(tweet_tokens[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Uaxwwkz6R-A",
        "outputId": "3a9ca4b6-7be4-4c25-ecf0-b0e372e6858c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('#FollowFriday', 'JJ'), ('@France_Inte', 'NNP'), ('@PKuchly57', 'NNP'), ('@Milipol_Paris', 'NNP'), ('for', 'IN'), ('being', 'VBG'), ('top', 'JJ'), ('engaged', 'VBN'), ('members', 'NNS'), ('in', 'IN'), ('my', 'PRP$'), ('community', 'NN'), ('this', 'DT'), ('week', 'NN'), (':)', 'NN')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 3 - normalize data**"
      ],
      "metadata": {
        "id": "ygp7ueDpOprN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def lemmatize_sentence(tokens):\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    lemmatized_sentence = []   \n",
        "    for word, tag in pos_tag(tokens):\n",
        "        if tag.startswith('NN'):\n",
        "            pos = 'n'\n",
        "        elif tag.startswith('VB'):\n",
        "            pos = 'v'\n",
        "        else:\n",
        "            pos = 'a'\n",
        "        lemmatized_sentence.append(lemmatizer.lemmatize(word, pos))\n",
        "    return lemmatized_sentence \n",
        "# print(lemmatize_sentence(tweet_tokens[0]))"
      ],
      "metadata": {
        "id": "ISQ4vCr26m11"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 4 - Removing data noises**"
      ],
      "metadata": {
        "id": "yjL2HUI9OkEB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_noise(tweet_tokens, stop_words = ()):\n",
        "    cleaned_tokens = []\n",
        "    for token, tag in pos_tag(tweet_tokens):\n",
        "        token = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+#]|[!*\\(\\),]|\\'(?:%[0-9a-fA-F][0-9a-fA-F]))+','', token)\n",
        "        token = re.sub(\"(@[A-Za-z0-9_]+)\",\"\", token)\n",
        "        if tag.startswith(\"NN\"):\n",
        "            pos = 'n'\n",
        "        elif tag.startswith('VB'):\n",
        "            pos = 'v'\n",
        "        else:\n",
        "            pos = 'a'\n",
        "        lemmatizer = WordNetLemmatizer()\n",
        "        token = lemmatizer.lemmatize(token, pos)\n",
        "        if len(token) > 0 and token not in string.punctuation and token.lower() not in stop_words:\n",
        "            cleaned_tokens.append(token.lower())\n",
        "    return cleaned_tokens"
      ],
      "metadata": {
        "id": "X9B6JYQ17aCH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = stopwords.words('english') \n",
        "print(remove_noise(tweet_tokens[0], stop_words))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F3YwyBmzNqw5",
        "outputId": "ccae1a57-26a4-4278-a299-a910fedf56a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['#followfriday', 'top', 'engage', 'member', 'community', 'week', ':)']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "positive_tweet_tokens = twitter_samples.tokenized('positive_tweets.json')\n",
        "negative_tweet_tokens = twitter_samples.tokenized('negative_tweets.json')\n",
        "positive_cleaned_tokens_list = []\n",
        "negative_cleaned_tokens_list = []\n",
        "for tokens in positive_tweet_tokens:\n",
        "    positive_cleaned_tokens_list.append(remove_noise(tokens, stop_words))\n",
        "for tokens in negative_tweet_tokens:\n",
        "    negative_cleaned_tokens_list.append(remove_noise(tokens, stop_words))\n",
        "\n",
        "print(positive_tweet_tokens[500]) \n",
        "print(positive_cleaned_tokens_list[500])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_tWEcx_e4aaB",
        "outputId": "83046e13-3107-4744-9712-9e4a146668b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Dang', 'that', 'is', 'some', 'rad', '@AbzuGame', '#fanart', '!', ':D', 'https://t.co/bI8k8tb9ht']\n",
            "['dang', 'rad', '#fanart', ':d']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 5 - Calculating words density**"
      ],
      "metadata": {
        "id": "V8ZbroevSP5o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_all_words(cleaned_tokens_list):\n",
        "    for tokens in cleaned_tokens_list:\n",
        "        for token in tokens:\n",
        "            yield token \n",
        "all_pos_words = get_all_words(positive_cleaned_tokens_list)"
      ],
      "metadata": {
        "id": "GdOFMTxtSRO_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "freq_dist_pos = FreqDist(all_pos_words)\n",
        "print(freq_dist_pos.most_common(10))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ywAi2qFQSWHx",
        "outputId": "4398c0ff-04e0-4e45-d4ab-e6947d3dbb25"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(':)', 3691), (':-)', 701), (':d', 658), ('thanks', 388), ('follow', 357), ('love', 333), ('...', 290), ('good', 283), ('get', 263), ('thank', 253)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 6 - Preparing data for the model**"
      ],
      "metadata": {
        "id": "6zXOpWORSeEA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_tweets_for_model(cleaned_tokens_list):\n",
        "    for tweet_tokens in cleaned_tokens_list:\n",
        "        yield dict([token, True] for token in tweet_tokens) \n",
        "positive_tokens_for_model = get_tweets_for_model(positive_cleaned_tokens_list)\n",
        "negative_tokens_for_model = get_tweets_for_model(negative_cleaned_tokens_list)"
      ],
      "metadata": {
        "id": "NbOXBGHiSf-v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "positive_dataset = [(tweet_dict, \"Positive\")for tweet_dict in positive_tokens_for_model] \n",
        "negative_dataset = [(tweet_dict, \"Negative\")for tweet_dict in negative_tokens_for_model] \n",
        "dataset = positive_dataset + negative_dataset \n",
        "random.shuffle(dataset) \n",
        "train_data = dataset[:7000]\n",
        "test_data = dataset[7000:]"
      ],
      "metadata": {
        "id": "fi5brLI0Skj8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 7 - Create and test the model**"
      ],
      "metadata": {
        "id": "yL9MGIUZSrHa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "classifier = NaiveBayesClassifier.train(train_data) \n",
        "print(\"Accuracy is:\", classify.accuracy(classifier, test_data)) \n",
        "print(classifier.show_most_informative_features(10))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E2V2DPfMSr80",
        "outputId": "cb86da7c-75af-4fe0-b31e-d7f5d9b1ae03"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy is: 0.994\n",
            "Most Informative Features\n",
            "                      :( = True           Negati : Positi =   2047.5 : 1.0\n",
            "                      :) = True           Positi : Negati =   1002.1 : 1.0\n",
            "                     sad = True           Negati : Positi =     55.2 : 1.0\n",
            "                follower = True           Positi : Negati =     37.3 : 1.0\n",
            "                    glad = True           Positi : Negati =     23.9 : 1.0\n",
            "                  arrive = True           Positi : Negati =     19.2 : 1.0\n",
            "                     bam = True           Positi : Negati =     18.5 : 1.0\n",
            "                 welcome = True           Positi : Negati =     16.3 : 1.0\n",
            "                     ugh = True           Negati : Positi =     15.5 : 1.0\n",
            "              appreciate = True           Positi : Negati =     14.5 : 1.0\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "custom_tweet = \"I ordered just once from TerribleCo, they screwed up, never used the app again.\"\n",
        "custom_tokens = remove_noise(word_tokenize(custom_tweet)) \n",
        "print(classifier.classify(dict([token, True] for token in custom_tokens)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LxRrpTZLS3_q",
        "outputId": "c0c8bf35-f319-45ed-bd74-69e0d53fbe99"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Negative\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "custom_tweet = 'Congrats #SportStar on your 7th best goal from last season winning goal of the year :) #Baller #Topbin #oneofmanyworldies'\n",
        "custom_tokens = remove_noise(word_tokenize(custom_tweet)) \n",
        "print(classifier.classify(dict([token, True] for token in custom_tokens)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hoK3xhYLS_My",
        "outputId": "9abb8e91-b363-43e7-cbe6-5ff7db00da71"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Positive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "custom_tweet = 'Thank you for sending my baggage to CityX and flying me to CityY at the same time. Brilliant service. #thanksGenericAirline'\n",
        "custom_tokens = remove_noise(word_tokenize(custom_tweet)) \n",
        "print(classifier.classify(dict([token, True] for token in custom_tokens)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yIzgAuTkTHqI",
        "outputId": "7f22c0cb-0cb9-4f6a-96dc-defe23f1103a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Positive\n"
          ]
        }
      ]
    }
  ]
}